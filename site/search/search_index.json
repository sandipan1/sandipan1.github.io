{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Who am I","text":"<p>Hi, I\u2019m Sandipan Haldar. I help businesses build and deploy AI systems that drive efficiency, automation, and smarter decision-making. I specialize in LLM applications, RAG, and AI-powered software solutions to enhance productivity, reduce costs, and unlock new opportunities. I write about AI engineering, practical AI deployment, and software development </p>"},{"location":"newsletter.html","title":"Newsletter","text":"<p>Once a month, I send out a newsletter with some of the things I've been working on, thinking about and learning about.</p> <p>If you'd like to recieve it, you can sign up below.</p>"},{"location":"work.html","title":"Work With Me","text":"<p>I'd love to have a chat if you're</p> <ul> <li>A startup struggling to figure out how to build more reliable evaluations for your LLM application</li> <li>working in the LLM space and would like to bounce some ideas about UX, Evals, Synthetic data or anything that's interesting to you</li> <li>Someone new to the LLM space and looking for some general advice</li> </ul> <p>Here's a cal.com link to book some time if this sounds like something you're interested with.</p> <p>In exchange, I might write about some of the issues and problems we talk about - without giving specific details. If I do use content we discussed about, I'll clear it with you before publishing anything.</p> <p>Reach me out at &lt;&gt;</p>"},{"location":"blog/stop-tweaking-promptsfix-your-retrieval-and-win.html","title":"Stop Tweaking Prompts\u2014Fix Your Retrieval and Win","text":""},{"location":"blog/stop-tweaking-promptsfix-your-retrieval-and-win.html#the-hidden-business-metrics-driving-ai-success-or-costly-failure","title":"The Hidden Business Metrics Driving AI Success (or Costly Failure)","text":"<p>Are your AI generation systems consistently underperforming despite endless prompt tweaking? You're likely optimizing the wrong part of your pipeline. After analyzing  generation systems across industries, I've discovered an uncomfortable truth: most teams waste resources fine-tuning models and prompts while ignoring the foundation everything depends on\u2014retrieval quality.</p>"},{"location":"blog/stop-tweaking-promptsfix-your-retrieval-and-win.html#the-costly-mistake-most-ai-teams-make","title":"The Costly Mistake Most AI Teams Make","text":"<p>Your generation task is only as good as the information it works with. Yet most teams spend weeks optimizing LLM prompts while dedicating minimal effort to their retrieval systems. This fundamental mistake costs companies thousands in wasted compute and engineering hours.</p>"},{"location":"blog/stop-tweaking-promptsfix-your-retrieval-and-win.html#common-tradeoffs-between-content-generation-retrieval","title":"Common Tradeoffs Between Content Generation &amp; Retrieval","text":"<ul> <li>Speed: Content generation takes 1-10 seconds per test, while retrieval takes only 10-800ms.</li> <li>Cost: Content generation can cost hundreds per run, whereas retrieval is negligible.</li> <li>Objectivity: Content evaluation is subjective, while retrieval metrics are quantitative.</li> <li>Iteration Speed: Content tuning takes hours, retrieval tuning takes minutes.</li> <li>Scalability: Content evaluation is limited, retrieval can be automated.</li> </ul> <p>By focusing on retrieval first, you build a solid foundation that enhances everything else. Here\u2019s why it matters and how to implement it systematically.</p>"},{"location":"blog/stop-tweaking-promptsfix-your-retrieval-and-win.html#framing-generation-as-a-retrieval-problem","title":"Framing Generation as a Retrieval Problem","text":"<p>Consider text-to-SQL generation. Before generating SQL, the system must first retrieve relevant context efficiently:</p> <p>The fundamental step is to create a dataset where each entry consists of an generated input (e.g., a prompt, a question) paired with its desired output (e.g., an answer, a summary, sql snippet). Now task of the retrieval system is to retrieve the SQL snippet from a query efficiently and accurately. By defining and tracking retrieval metrics like recall , MRR you can identify areas where the retrieval system is failing to find the correct SQL snippets for cetain type of queries which informs optimization effort like finetuning embedding model, experiment with different indexing</p>"},{"location":"blog/stop-tweaking-promptsfix-your-retrieval-and-win.html#key-benefits-of-retrieval-first-approach","title":"Key Benefits of Retrieval-First Approach","text":"<ol> <li>Early edge case detection: Identify potential failure points by analyzing retrieval errors before they impact production performance.</li> <li>Business logic integration: Ensure that company-specific calculations, such as custom pricing rules or compliance constraints, are accurately retrieved and incorporated into query generation. For example, when handling tax computations, retrieving company-specific tax codes before generating SQL ensures compliance and accuracy:</li> </ol> <pre><code># Example tax code for a specific company\ndef calculate_tax(amount, tax_code):\n    tax_rates = {\"A\": 0.05, \"B\": 0.1, \"C\": 0.15}  # Different tax rules per company\n    return amount * tax_rates.get(tax_code, 0.08)  # Default tax rate if code is unknown\n\n# Applying tax calculation in query generation\nfinal_query = f\"SELECT total_price, total_price + {calculate_tax('total_price', tax_code)} AS total_with_tax FROM orders\"\n</code></pre> <ol> <li>Metrics-driven optimization: Precision, recall, and MRR provide objective benchmarks and help ensure accurate and comprehensive infromation retrieval\u00a0</li> <li>Few-shot enhancement: A generation task can be greatly improved by including few-shot examples especially those of business specific edge cases or difficult to retrieve.</li> </ol>"},{"location":"blog/stop-tweaking-promptsfix-your-retrieval-and-win.html#the-three-step-system-for-optimizing-retrieval","title":"The Three-Step System for Optimizing Retrieval","text":""},{"location":"blog/stop-tweaking-promptsfix-your-retrieval-and-win.html#step-1-create-synthetic-test-data","title":"Step 1: Create Synthetic Test Data","text":"<p>Generate diverse test questions reflecting real user queries:</p> <p>Example input:</p> <pre><code>challenging_examples = [\"SELECT * FROM sales WHERE amount &gt; 1000\", \"SELECT COUNT(*) FROM users WHERE signup_date &gt; '2024-01-01'\"]\n</code></pre> <p>Example output:</p> <pre><code>[{\n    \"question\": \"How many users signed up after January 1, 2024?\",\n    \"relevance\": \"high\"\n}, {\n    \"question\": \"Show all sales transactions where the amount exceeds 1000.\",\n    \"relevance\": \"high\"\n}]\n</code></pre> <pre><code>from pydantic import BaseModel\nimport instructor\nimport random\n\nclass SyntheticQuestion(BaseModel):\n    question: str\n    relevance: str\n\nclient = instructor.from_openai(openai.OpenAI())\nquestions = []\n\nfor sql_example in challenging_examples:\n    constraints = random.choice([\"time\", \"amount\", \"limit\", \"comparison\"])\n    question = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        response_model=SyntheticQuestion,\n        messages=[\n            {\"role\": \"user\", \"content\":f\"Generate a question for this SQL: {sql_example}. Add {constraints} constraint.\"}\n            ]\n    )\n    questions.append(question)\n</code></pre> <p>This ensures comprehensive test coverage without needing real user data.</p>"},{"location":"blog/stop-tweaking-promptsfix-your-retrieval-and-win.html#step-2-define-meaningful-retrieval-metrics","title":"Step 2: Define Meaningful Retrieval Metrics","text":"<ul> <li>Precision@K: How many retrieved items are relevant?    Improving precision makes sure we are not wasting resources and increasing latency by processing information which are irrelevant</li> <li>Recall@K: How many relevant items did we retrieve?\u00a0   By optimizing for recall, we ensure that the language model has access to all necessary information, which can lead to more accurate and reliable generated responses.</li> <li>MRR@K (Mean Reciprocal Rank): How highly ranked are the relevant items?   If we want to display retrieved results as citations to users, we only show the highly ranked retrieved results to the user. </li> </ul> <p>For most generation tasks, recall is critical\u2014your LLM needs all necessary information to generate accurate responses.</p>"},{"location":"blog/stop-tweaking-promptsfix-your-retrieval-and-win.html#step-3-build-your-evaluation-pipeline","title":"Step 3: Build Your Evaluation Pipeline","text":"<p>For example, you want to improve performance and accuracy of your text-to-SQL system for your analytics engine.</p> <p>You want to evaluate if adding a reranker is worth it.  While it promised better results,\u00a0it adds 200ms latency and $0.01 per query.</p> <p>You need a way to systematically improve your system with data driven methods so that you are not wasting months on the wrong thing.\u00a0</p> <p>Set up an automated pipeline to test different retrieval strategies:</p> <pre><code># Compare different retrieval approaches systematically\nfor embedding_model in [\"text-embedding-3-small\", \"text-embedding-3-large\"]:\n    for search_type in [\"vector\", \"hybrid\"]:\n        for use_reranker in [True, False]:\n            results = evaluate_retrieval(\n                questions,\n                embedding_model=embedding_model,\n                search_type=search_type,\n                use_reranker=use_reranker\n            )\n            log_metrics(results)\n</code></pre> <p>Finding from the evaluation pipeline :\u00a0</p> <ol> <li>Upgrading from text-embedding-3-small to text-embedding-3-large provided better performance than adding a re-ranker.</li> <li>The larger embedding model achieved near-perfect recall (0.98) at k=20 without a re-ranker.</li> <li>The re-ranker showed minimal statistical improvement, confirmed using bootstrapping and t-tests.We avoided an unnecessary cost while improving overall performance. </li> </ol> <p>This allows for evidence-based decisions about retrieval architecture.</p>"},{"location":"blog/stop-tweaking-promptsfix-your-retrieval-and-win.html#validating-your-improvements-with-statistical-rigor","title":"Validating Your Improvements with Statistical Rigor","text":"<p>Once you see performance improvements, validate them statistically:</p> <ul> <li>Bootstrap sampling: Simulate multiple runs to estimate result stability.</li> <li>Confidence intervals: Visualize performance ranges to identify overlapping results.</li> <li>Statistical testing: Use t-tests to confirm if differences are significant.</li> </ul> <p>When comparing embedding models, t-statistics below -10 and p-values &lt; 0.001 confirmed that the larger model's superior performance was statistically significant.</p>"},{"location":"blog/stop-tweaking-promptsfix-your-retrieval-and-win.html#three-critical-mistakes-to-avoid","title":"Three Critical Mistakes to Avoid","text":"<ol> <li>Starting with generation evaluation: This is slow, expensive, and subjective. Start with retrieval metrics for faster iteration.</li> <li>Using narrow test data: Generate diverse synthetic questions to uncover blind spots.</li> <li>Making decisions without statistical validation: Ensure improvements are real, not just random variation.</li> </ol>"},{"location":"blog/stop-tweaking-promptsfix-your-retrieval-and-win.html#start-measuring-what-matters","title":"Start Measuring What Matters","text":"<p>Retrieval isn\u2019t just a nice-to-have\u2014it\u2019s the backbone of effective generation tasks. Whether you\u2019re summarizing, reporting, or extracting insights, it\u2019s the step that turns potential into performance</p> <p>The most profitable AI systems aren\u2019t built on hype\u2014they\u2019re built on systematic measurement and optimization that drive real business impact. By prioritizing retrieval, you will create AI generation systems that are faster, more reliable, and cost-effective.</p> <p>Don't let poor retrieval undermine your AI projects. Start optimizing today.</p>"},{"location":"blog/archive/2025.html","title":"2025","text":""},{"location":"blog/category/machine-learning.html","title":"Machine Learning","text":""},{"location":"blog/category/ai-engineering.html","title":"AI Engineering","text":""},{"location":"blog/category/llm.html","title":"LLM","text":""},{"location":"blog/category/rag.html","title":"RAG","text":""}]}
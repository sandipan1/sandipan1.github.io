{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Who am I","text":"<p>You\u2019re not just looking for another consultant. You need someone who\u2019s been in the trenches.</p> <p>I\u2019m Sandipan Haldar, an Applied AI Consultant who\u2019s spent 4+ years designing data pipelines and deploying models for companies like HRS and fast-growing startups. While leading AI projects, I noticed a pattern: businesses either treat AI like a software sprint or chase technical metrics that don\u2019t move the needle. Teams burn cycles on:</p> <ul> <li> <p>Debating solutions instead of testing them, while customers churn over slow, hallucination-prone AI</p> </li> <li> <p>Building \"impressive\" features users don\u2019t need (like training models to \u201ctalk like pirates\u201d) while latency drives users away</p> </li> <li> <p>Organizing data in ways that sabotage security and recall (like the \"big pile of records\" that leaks user data)</p> </li> <li> <p>Optimizing RAG systems backwards - using bigger embedding models/rerankers without first asking \u201cWhat are users actually asking? </p> </li> <li> <p>Rotating around different tools and not make any real progress</p> </li> </ul> <p>That\u2019s why I shifted to consulting\u2014to help companies like yours:</p> <ul> <li> <p>Stop losing customers with RAG systems that balance accuracy, speed, and compliance</p> </li> <li> <p>Cut decision paralysis by replacing endless debates with clear evaluation frameworks</p> </li> <li> <p>Turn engineers into AI leaders in weeks, not years, using hand-on experience</p> </li> <li> <p>Fix hidden bottlenecks causing latency and hallucinations, from broken data pipelines to untracked model drift</p> </li> <li> <p>Turn technical wins into business outcomes: Like the client who used AI to prevent $50k/month in communication delays (not just \u201cimproved recall\u201d)</p> </li> </ul> <p>I don't just give advice \u2014 I do the work too  See How It Works \u2192 </p> Contact Me Book a free call"},{"location":"newsletter.html","title":"Newsletter","text":"<p>Once a month, I send out a newsletter with some of the things I've been working on, thinking about and learning about.</p> <p>If you'd like to recieve it, you can sign up below.</p>"},{"location":"work.html","title":"Work With Me","text":"<p>I help in the following areas : </p>"},{"location":"work.html#data-strategy","title":"Data Strategy","text":"<ul> <li>Analyse your existing data landscape and your user's information needs</li> <li>Audit existing pipelines for hidden security risks</li> <li>Architect and implement granular indexing strategies for your specific usecases</li> <li>Design and deploy robust query routing strategies</li> <li>End-to-end fine-tuning pipelines for embedding models for highly personalized retrieval</li> <li>Identify bottlenecks and performance optimization for future proof systems</li> </ul>"},{"location":"work.html#team-building","title":"Team building","text":"<ul> <li>Stand-ups become focused on concrete experiments and metrics that is aligned with your business goals</li> <li>Hand-on training on AI Expertise for your engineering team</li> <li>Roadmap design and alignment</li> <li>Communication strategies for probabilistic systems unlike traditional software systems</li> </ul>"},{"location":"work.html#reliable-evaluation-system","title":"Reliable Evaluation system","text":"<ul> <li>Implement the right evaluation metrics that aligns with your business needs </li> <li>Accelerate your testing from weeks to days </li> <li>Align LLM judges with human feedback</li> <li>Implement statistical validation to distinguish meaningful improvements from random noise</li> <li>Benchmark architectures to quantify tradeoffs between cost, latency, and accuracy</li> <li>Synthetic test data to systematically target edge cases and failure modes without exposing sensitive user data</li> <li>Optimize experiment costs \u2013 Cache API results, test on subsets first, and prune dead-end experiments early</li> </ul>"},{"location":"work.html#improve-rag-systems","title":"Improve RAG systems","text":"<ul> <li>Pinpoint retrieval weaknesses by segmenting user queries by topic, intent to diagonise system failure</li> <li>Structure data \u2013 Extract metadata, tables, and visual context to enable precise filtering.</li> <li>Automatically direct requests to specialized tools (SQL engines, image indexes, codebases)</li> <li>Fine-tune embeddings and re-rankers using real user feedback to align outputs with domain-specific needs.</li> <li>Build user-driven validation (thumbs-up/down, error highlighting) and guardrails for systematic upgrades.</li> </ul>"},{"location":"work.html#who-this-is-for","title":"Who This Is For:","text":"<ul> <li>Startups losing customers to unreliable AI</li> <li>Teams struggling with slow, untrustworthy apps</li> <li>Engineering leaders tired of \u201cWhy are we still debating this?\u201d meetings</li> </ul> <p>Here's a cal.com link to book some time if this sounds like something you're interested with.</p> <p>In exchange, I might write about some of the issues and problems we talk about - without giving specific details. If I do use content we discussed about, I'll clear it with you before publishing anything.</p> <p>Reach out to me !</p>"},{"location":"blog/stop-tweaking-promptsfix-your-retrieval-and-win.html","title":"Stop Tweaking Prompts\u2014Fix Your Retrieval and Win","text":""},{"location":"blog/stop-tweaking-promptsfix-your-retrieval-and-win.html#the-hidden-business-metrics-driving-ai-success-or-costly-failure","title":"The Hidden Business Metrics Driving AI Success (or Costly Failure)","text":"<p>Are your AI generation systems consistently underperforming despite endless prompt tweaking? You're likely optimizing the wrong part of your pipeline. After analyzing  generation systems across industries, I've discovered an uncomfortable truth: most teams waste resources fine-tuning models and prompts while ignoring the foundation everything depends on\u2014retrieval quality.</p>"},{"location":"blog/stop-tweaking-promptsfix-your-retrieval-and-win.html#the-costly-mistake-most-ai-teams-make","title":"The Costly Mistake Most AI Teams Make","text":"<p>Your generation task is only as good as the information it works with. Yet most teams spend weeks optimizing LLM prompts while dedicating minimal effort to their retrieval systems. This fundamental mistake costs companies thousands in wasted compute and engineering hours.</p>"},{"location":"blog/stop-tweaking-promptsfix-your-retrieval-and-win.html#common-tradeoffs-between-content-generation-retrieval","title":"Common Tradeoffs Between Content Generation &amp; Retrieval","text":"<ul> <li>Speed: Content generation takes 1-10 seconds per test, while retrieval takes only 10-800ms.</li> <li>Cost: Content generation can cost hundreds per run, whereas retrieval is negligible.</li> <li>Objectivity: Content evaluation is subjective, while retrieval metrics are quantitative.</li> <li>Iteration Speed: Content tuning takes hours, retrieval tuning takes minutes.</li> <li>Scalability: Content evaluation is limited, retrieval can be automated.</li> </ul> <p>By focusing on retrieval first, you build a solid foundation that enhances everything else. Here\u2019s why it matters and how to implement it systematically.</p>"},{"location":"blog/stop-tweaking-promptsfix-your-retrieval-and-win.html#framing-generation-as-a-retrieval-problem","title":"Framing Generation as a Retrieval Problem","text":"<p>Consider text-to-SQL generation. Before generating SQL, the system must first retrieve relevant context efficiently:</p> <p>The fundamental step is to create a dataset where each entry consists of an generated input (e.g., a prompt, a question) paired with its desired output (e.g., an answer, a summary, sql snippet). Now task of the retrieval system is to retrieve the SQL snippet from a query efficiently and accurately. By defining and tracking retrieval metrics like recall , MRR you can identify areas where the retrieval system is failing to find the correct SQL snippets for cetain type of queries which informs optimization effort like finetuning embedding model, experiment with different indexing</p>"},{"location":"blog/stop-tweaking-promptsfix-your-retrieval-and-win.html#key-benefits-of-retrieval-first-approach","title":"Key Benefits of Retrieval-First Approach","text":"<ol> <li>Early edge case detection: Identify potential failure points by analyzing retrieval errors before they impact production performance.</li> <li>Business logic integration: Ensure that company-specific calculations, such as custom pricing rules or compliance constraints, are accurately retrieved and incorporated into query generation. For example, when handling tax computations, retrieving company-specific tax codes before generating SQL ensures compliance and accuracy:</li> </ol> <pre><code># Example tax code for a specific company\ndef calculate_tax(amount, tax_code):\n    tax_rates = {\"A\": 0.05, \"B\": 0.1, \"C\": 0.15}  # Different tax rules per company\n    return amount * tax_rates.get(tax_code, 0.08)  # Default tax rate if code is unknown\n\n# Applying tax calculation in query generation\nfinal_query = f\"SELECT total_price, total_price + {calculate_tax('total_price', tax_code)} AS total_with_tax FROM orders\"\n</code></pre> <ol> <li>Metrics-driven optimization: Precision, recall, and MRR provide objective benchmarks and help ensure accurate and comprehensive infromation retrieval\u00a0</li> <li>Few-shot enhancement: A generation task can be greatly improved by including few-shot examples especially those of business specific edge cases or difficult to retrieve.</li> </ol>"},{"location":"blog/stop-tweaking-promptsfix-your-retrieval-and-win.html#the-three-step-system-for-optimizing-retrieval","title":"The Three-Step System for Optimizing Retrieval","text":""},{"location":"blog/stop-tweaking-promptsfix-your-retrieval-and-win.html#step-1-create-synthetic-test-data","title":"Step 1: Create Synthetic Test Data","text":"<p>Generate diverse test questions reflecting real user queries:</p> <p>Example input:</p> <pre><code>challenging_examples = [\"SELECT * FROM sales WHERE amount &gt; 1000\", \"SELECT COUNT(*) FROM users WHERE signup_date &gt; '2024-01-01'\"]\n</code></pre> <p>Example output:</p> <pre><code>[{\n    \"question\": \"How many users signed up after January 1, 2024?\",\n    \"relevance\": \"high\"\n}, {\n    \"question\": \"Show all sales transactions where the amount exceeds 1000.\",\n    \"relevance\": \"high\"\n}]\n</code></pre> <pre><code>from pydantic import BaseModel\nimport instructor\nimport random\n\nclass SyntheticQuestion(BaseModel):\n    question: str\n    relevance: str\n\nclient = instructor.from_openai(openai.OpenAI())\nquestions = []\n\nfor sql_example in challenging_examples:\n    constraints = random.choice([\"time\", \"amount\", \"limit\", \"comparison\"])\n    question = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        response_model=SyntheticQuestion,\n        messages=[\n            {\"role\": \"user\", \"content\":f\"Generate a question for this SQL: {sql_example}. Add {constraints} constraint.\"}\n            ]\n    )\n    questions.append(question)\n</code></pre> <p>This ensures comprehensive test coverage without needing real user data.</p>"},{"location":"blog/stop-tweaking-promptsfix-your-retrieval-and-win.html#step-2-define-meaningful-retrieval-metrics","title":"Step 2: Define Meaningful Retrieval Metrics","text":"<ul> <li>Precision@K: How many retrieved items are relevant?    Improving precision makes sure we are not wasting resources and increasing latency by processing information which are irrelevant</li> <li>Recall@K: How many relevant items did we retrieve?\u00a0   By optimizing for recall, we ensure that the language model has access to all necessary information, which can lead to more accurate and reliable generated responses.</li> <li>MRR@K (Mean Reciprocal Rank): How highly ranked are the relevant items?   If we want to display retrieved results as citations to users, we only show the highly ranked retrieved results to the user. </li> </ul> <p>For most generation tasks, recall is critical\u2014your LLM needs all necessary information to generate accurate responses.</p>"},{"location":"blog/stop-tweaking-promptsfix-your-retrieval-and-win.html#step-3-build-your-evaluation-pipeline","title":"Step 3: Build Your Evaluation Pipeline","text":"<p>For example, you want to improve performance and accuracy of your text-to-SQL system for your analytics engine.</p> <p>You want to evaluate if adding a reranker is worth it.  While it promised better results,\u00a0it adds 200ms latency and $0.01 per query.</p> <p>You need a way to systematically improve your system with data driven methods so that you are not wasting months on the wrong thing.\u00a0</p> <p>Set up an automated pipeline to test different retrieval strategies:</p> <pre><code># Compare different retrieval approaches systematically\nfor embedding_model in [\"text-embedding-3-small\", \"text-embedding-3-large\"]:\n    for search_type in [\"vector\", \"hybrid\"]:\n        for use_reranker in [True, False]:\n            results = evaluate_retrieval(\n                questions,\n                embedding_model=embedding_model,\n                search_type=search_type,\n                use_reranker=use_reranker\n            )\n            log_metrics(results)\n</code></pre> <p>Finding from the evaluation pipeline :\u00a0</p> <ol> <li>Upgrading from text-embedding-3-small to text-embedding-3-large provided better performance than adding a re-ranker.</li> <li>The larger embedding model achieved near-perfect recall (0.98) at k=20 without a re-ranker.</li> <li>The re-ranker showed minimal statistical improvement, confirmed using bootstrapping and t-tests.We avoided an unnecessary cost while improving overall performance. </li> </ol> <p>This allows for evidence-based decisions about retrieval architecture.</p>"},{"location":"blog/stop-tweaking-promptsfix-your-retrieval-and-win.html#validating-your-improvements-with-statistical-rigor","title":"Validating Your Improvements with Statistical Rigor","text":"<p>Once you see performance improvements, validate them statistically:</p> <ul> <li>Bootstrap sampling: Simulate multiple runs to estimate result stability.</li> <li>Confidence intervals: Visualize performance ranges to identify overlapping results.</li> <li>Statistical testing: Use t-tests to confirm if differences are significant.</li> </ul> <p>When comparing embedding models, t-statistics below -10 and p-values &lt; 0.001 confirmed that the larger model's superior performance was statistically significant.</p>"},{"location":"blog/stop-tweaking-promptsfix-your-retrieval-and-win.html#three-critical-mistakes-to-avoid","title":"Three Critical Mistakes to Avoid","text":"<ol> <li>Starting with generation evaluation: This is slow, expensive, and subjective. Start with retrieval metrics for faster iteration.</li> <li>Using narrow test data: Generate diverse synthetic questions to uncover blind spots.</li> <li>Making decisions without statistical validation: Ensure improvements are real, not just random variation.</li> </ol>"},{"location":"blog/stop-tweaking-promptsfix-your-retrieval-and-win.html#start-measuring-what-matters","title":"Start Measuring What Matters","text":"<p>Retrieval isn\u2019t just a nice-to-have\u2014it\u2019s the backbone of effective generation tasks. Whether you\u2019re summarizing, reporting, or extracting insights, it\u2019s the step that turns potential into performance</p> <p>The most profitable AI systems aren\u2019t built on hype\u2014they\u2019re built on systematic measurement and optimization that drive real business impact. By prioritizing retrieval, you will create AI generation systems that are faster, more reliable, and cost-effective.</p> <p>Don't let poor retrieval undermine your AI projects. Start optimizing today.</p>"},{"location":"blog/archive/2025.html","title":"2025","text":""},{"location":"blog/category/machine-learning.html","title":"Machine Learning","text":""},{"location":"blog/category/ai-engineering.html","title":"AI Engineering","text":""},{"location":"blog/category/llm.html","title":"LLM","text":""},{"location":"blog/category/rag.html","title":"RAG","text":""}]}